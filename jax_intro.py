# -*- coding: utf-8 -*-
"""jax intro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10KKI2sj7ioabYJZVQ7qOb43vE6fy91aQ
"""

!pip install equinox

import jax
import numpy as np
import jax.numpy as jnp
import jax.random as random
import jax.lax as lax
from jax.nn.initializers import glorot_normal
#import equinox as eqx
from functools import partial
import optax
from sklearn.metrics import accuracy_score
from torchvision import datasets, transforms

# Simple nn from scratch with jax
class NeuralNetwork():
  # With this nn we will let the user choose # of layers desired
  def __init__(self, layer_sizes):
    self.layer_sizes = layer_sizes

  # initialize parameters
  # Created a nested dictionary of parameters based on # of layers
  def initialize_params(self, key):
    params = {}
    for i in range(len(self.layer_sizes) - 1):
      key, wkey = jax.random.split(key, 2)
      params[f'layer_{i+1}'] = {}
      # following torch convention: x @ W.T + b, so initialize with next layer shape first
      params[f'layer_{i+1}']['weights'] = glorot_normal()(wkey, (self.layer_sizes[i + 1], self.layer_sizes[i]))
      params[f'layer_{i+1}']['bias'] = jnp.zeros((self.layer_sizes[i + 1], ))
    return params

  # Forward pass taking initialized params and input data
  # For jit compiling to work, array shapes (not values) must be fixed at compile time, i.e.
  # array shapes can't be dependent on runtime values
  # jit will also mark self as an arg, so we need to make it a static arg
  @partial(jax.jit, static_argnums=(0,))
  def forward(self, params, x):
    params_items = list(params.items())
    for layer, wandbs in params_items[:-1]:
      # using relu activation for all layers except last
      x = jax.nn.relu(x@wandbs['weights'].T + wandbs['bias'])

    # no activation on final layer
    x = x@params_items[-1][1]['weights'].T + params_items[-1][1]['bias']
    return x # return logits for loss fn

  @partial(jax.jit, static_argnums=(0,))
  def loss_fn(self, params, x, y):
    logits = self.forward(params, x)
    loss = optax.softmax_cross_entropy(logits, y)
    return jnp.mean(loss)

  def train(self, params, x, y, lr, epochs):
    for epoch in range(epochs):
      params = self.train_step(params, x, y, lr)
    return params

  # Separate training step from loop to JIT efficiently
  @partial(jax.jit, static_argnums=(0, 4))
  def train_step(self, params, x, y, lr):
    #JAX works with pytrees
    #PyTrees are basically nested structures like dicts, lists, lists of lists,
    #whos leaves contain jax arrays/jax compatible data types
    #In my case, params is a pytree: nested dictionary containing values (leaves)
    #that are jax arrays

    #Jax grad has support for PyTrees, so we can use grad to get the gradients
    #for all params, along with a little tree mapping for updating the params
    grads = jax.grad(self.loss_fn)(params, x, y)
    params = jax.tree.map(lambda p, g: p - lr * g, params, grads)

    return params

  @partial(jax.jit, static_argnums=(0,))
  def predict(self, params, x):
    logits = self.forward(params, x)
    return jax.nn.softmax(logits, -1)

# Trying NN on MNIST
transform = transforms.Compose([transforms.ToTensor()])

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

x_train = jnp.array(train_dataset.data.numpy().reshape(-1, 784) / 255)
y_train = jnp.array(train_dataset.targets.numpy())

x_test = jnp.array(test_dataset.data.numpy().reshape(-1, 784) / 255)
y_test = jnp.array(test_dataset.targets.numpy())

num_classes = 10
y_train_one_hot = jax.nn.one_hot(y_train, num_classes=num_classes)
y_test_one_hot = jax.nn.one_hot(y_test, num_classes=num_classes)

mnist_nn = NeuralNetwork([784, 128, 10])
params = mnist_nn.initialize_params(jax.random.key(42))
params = mnist_nn.train(params, x_train, y_train_one_hot, 0.01, 73)
y_pred = mnist_nn.predict(params, x_test).argmax(1)

accuracy_score(y_test, y_pred)

y = np.arange(1, 10, 2)
y[0] = 2
y

x = jnp.arange(1, 11)
x = x.at[0].set(2)
x = x.reshape(5, 2).T
x.sharding

@jax.jit
def jit_multiply(a, b):
  return a @ b.T

def multiply(a, b):
  return a @ b.T

tensor1 = jnp.arange(1, 1001).reshape(50, 20)
tensor2 = jnp.arange(1, 1001).reshape(50, 20)

# Commented out IPython magic to ensure Python compatibility.
# %timeit jit_multiply(tensor1, tensor2).block_until_ready()

# Commented out IPython magic to ensure Python compatibility.
# %timeit multiply(tensor1, tensor2).block_until_ready()

print(jax.make_jaxpr(jit_multiply)(tensor1, tensor2))

@jax.jit
def f(x, y):
  print("Running f():")
  print(f"  {x = }")
  print(f"  {y = }")
  result = jnp.dot(x + 1, y + 1)
  print(f"  {result = }")
  return result

a = np.random.randn(3, 4)
b = np.random.randn(4,)
f(a,b)
# jax jit compiling treats all objects as tracers
# tracer objects are basically placeholders
# they describe array shape and dtype, but not the actual values

def f(x, neg):
  return -x if neg else x

f = jax.jit(f, static_argnums=(1,))

f(jnp.array([-1, 2]), True)

@jax.vmap
@jax.grad
@jax.jit
def grad_f(x):
  return x**2 + 2*x + 3

grad_f(jnp.array([1., 2., 3.]))

key = random.key(42)
key2 = random.key(42)
key, key2
# JAX keys are basically immutable random states that are only changed explicitly
# using the same random key on two tensors will produce two identical tensors

arr1 = random.normal(key)
subkeys = random.split(key, 2)
arr1
subkeys

subkeys[-5]

key = random.key(42)
subkeys = random.split(key, 2)
print("subkeys shape:", subkeys.shape)
print("subkeys:", subkeys)

# These should work:
arr2 = random.normal(subkeys[0])
arr3 = random.normal(subkeys[1])

# These should fail:
try:
    arr4 = random.normal(subkeys[2])
    print("subkeys[2] worked - this is unexpected!")
except IndexError:
    print("subkeys[2] failed as expected")

def g(x):

  return jnp.where(x > 2, x, 2 * x)

g(2.)

@jax.jit
def relu(x):
  return jnp.where(x > 0, x, 0)

key = random.key(67)
test_arr = random.normal(key, (6, 7), dtype=np.float32)

relu(test_arr)

@partial(jax.jit, static_argnums=1)
def g_jitted(x, n):
  i = 0
  while i < n:
    i += 1
  return x + i

# partial lets you use jit as a decorator with static args

g_jitted(10, 20)

x = jnp.arange(25)
w = jnp.array([2., 3., 4.])

def conv1d(x, w):
  output = []
  for i in range(len(x) - len(w) + 1):
    output.append(x[i:i+len(w)] @ w)
  return jnp.array(output)

print(x)
conv1d(x, w)

print(y)
print(w)

x = jnp.arange(20).reshape(4, 5)
w = jnp.arange(9).reshape(3, 3)

def conv2d(x, w):
  output = []
  for i in range(x.shape[0]-w.shape[0]+1):
    for j in range(x.shape[1]-w.shape[1]+1):
      output.append(jnp.sum(x[i:i+w.shape[0], j:j+w.shape[1]] * w))

  output_height = x.shape[0]-w.shape[0]+1
  output_width = x.shape[1]-w.shape[1]+1
  return jnp.array(output).reshape(output_height, output_width)
conv2d(x, w)

a = jnp.arange(9).reshape(3, 3)
b = jnp.arange(9).reshape(3, 3)

a.reshape(-1, ) @ b.reshape(-1, )

f = lambda x: x**3 + 2*x**2 + x + 3
jax.vmap(jax.grad(f))(jnp.array([1., 2., 3.]))

simple_nn = NeuralNetwork([6, 8, 7, 4, 2])
params = simple_nn.initialize_params(jax.random.key(42))
x_dummy = jax.random.normal(jax.random.key(0), shape=(10, 6))
y_true = jnp.array([
    [1, 0],  # class 0
    [0, 1],  # class 1
    [1, 0],  # class 0
    [0, 1],  # class 1
    [0, 1],  # class 1
    [1, 0],  # class 0
    [0, 1],  # class 1
    [1, 0],  # class 0
    [0, 1],  # class 1
    [1, 0]   # class 0
])
trained_params = simple_nn.train(params, x_dummy, y_true, 0.01, 10)
y_pred = simple_nn.predict(trained_params, x_dummy).argmax(1)
y_true = y_true.argmax(1)
accuracy_score(y_true, y_pred)