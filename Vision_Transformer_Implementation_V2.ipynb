{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ulQ6EfmmvjJT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "R74aqsziSMyd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, patch_size, embed_dim, img_h, img_w, in_channels):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = (img_h // patch_size) * (img_w // patch_size)\n",
        "    patch_dim = in_channels * (patch_size ** 2)\n",
        "    # stride is patch size to avoid overlapping kernels\n",
        "    self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
        "    self.proj = nn.Linear(in_features=patch_size**2 * in_channels, out_features=embed_dim) # project to input dim D of transformer\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    #B, C, H, W is current shape of tensor\n",
        "    B = x.shape[0]\n",
        "    patches = self.unfold(x).transpose(1, 2) # B, C * P^2, N (number of patches) -> # B, N, C * P^2\n",
        "    x = self.proj(patches) # project to transformer input dim\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1) #cls token for all samples\n",
        "    x = torch.cat([cls_tokens, x], dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    return x"
      ],
      "metadata": {
        "id": "AEcKYIfcS9bV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, d_model, eps=1e-6):\n",
        "    super().__init__()\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True) # keep dim to maintain tensor dimensionality\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.gamma * (x - mean) / (std + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "lWPEPItpF-8K"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.gelu(self.linear1(x))\n",
        "    x = self.dropout(x)\n",
        "    return self.linear2(x)"
      ],
      "metadata": {
        "id": "SuP0AvLeG-V-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_head = d_model // n_heads\n",
        "    self.W_qkv = nn.Linear(d_model, 3*d_model) # One matrix for q, k, v for computation efficiency\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "    self.d_model = d_model\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def ScaledDotProductAttention(self, q, k, v, mask=None):\n",
        "    d_k = k.size(-1) # Q and K have shape B, N_H, T, D_H\n",
        "    scores = q@k.transpose(-2, -1) / math.sqrt(d_k) # B, N_H, T, T\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attn = self.dropout(F.softmax(scores, dim=-1)) # use last dim because we want to sum across keys to get probs for each query\n",
        "    return attn@v # B, N_H, T, D_H\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, D = x.shape\n",
        "\n",
        "    qkv = self.W_qkv(x)\n",
        "\n",
        "    q, k, v = torch.split(qkv, self.d_model, dim=-1) # Split into q, k, v\n",
        "\n",
        "    q = q.reshape(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "    k = k.reshape(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "    v = v.reshape(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "    attn = self.ScaledDotProductAttention(q, k, v) # no causal mask for ViT\n",
        "    attn_concat = attn.transpose(1, 2).reshape(B, T, D)\n",
        "    mha_output = self.W_o(attn_concat)\n",
        "\n",
        "    return mha_output # shape (B, T, D)"
      ],
      "metadata": {
        "id": "U5ZHjiT9BvxS"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
        "    self.layernorm1 = LayerNorm(d_model)\n",
        "    self.layernorm2 = LayerNorm(d_model)\n",
        "    self.mlp = FeedForward(d_model, d_ff, dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.mha(self.layernorm1(x))\n",
        "    x = x + self.mlp(self.layernorm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "HPBjhPW7IvU6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, d_model, n_layers, n_heads, d_ff, dropout_rate, patch_size, num_classes, img_h, img_w, in_channels): # Need classes for classification head\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embedding = PatchEmbedding(patch_size, d_model, img_h, img_w, in_channels)\n",
        "\n",
        "    self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, d_ff, dropout_rate)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "    self.final_layernorm = LayerNorm(d_model)\n",
        "    self.mlp_head = nn.Linear(d_model, num_classes)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.patch_embedding(x)\n",
        "\n",
        "    for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x)\n",
        "\n",
        "    x = self.final_layernorm(x)\n",
        "\n",
        "    cls_token = x[:, 0, :]\n",
        "\n",
        "    # classification head\n",
        "    logits = self.mlp_head(cls_token)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "67eEePtnNVXA"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "def get_config(dataloader):\n",
        "  image_batch, _ = next(iter(dataloader))\n",
        "  B, C, H, W = image_batch.shape\n",
        "  return {\n",
        "      \"channels\": C,\n",
        "      \"height\": H,\n",
        "      \"width\": W,\n",
        "      \"num_classes\": len(dataloader.dataset.classes)\n",
        "  }\n",
        "\n",
        "config = get_config(train_loader)\n",
        "\n",
        "model = ViT(patch_size=4,\n",
        "            d_model=192,\n",
        "            n_layers=8,\n",
        "            n_heads=12,\n",
        "            d_ff=768,\n",
        "            dropout_rate=0.1,\n",
        "            num_classes=config[\"num_classes\"],\n",
        "            img_h=config[\"height\"],\n",
        "            img_w=config[\"width\"],\n",
        "            in_channels=config[\"channels\"]).to(device)\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 12\n",
        "for epoch in range(epochs):\n",
        "  # Training\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "\n",
        "  for i, (data, targets) in enumerate(train_loader):\n",
        "    data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    _, predicted = outputs.max(1)\n",
        "    train_total += targets.size(0)\n",
        "    train_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  # Validation\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "      data, targets = data.to(device), targets.to(device)\n",
        "      outputs = model(data)\n",
        "      val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "      # Calculate validation accuracy\n",
        "      _, predicted = outputs.max(1)\n",
        "      val_total += targets.size(0)\n",
        "      val_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  # Print epoch results\n",
        "  train_acc = 100. * train_correct / train_total\n",
        "  val_acc = 100. * val_correct / val_total\n",
        "  avg_train_loss = train_loss / len(train_loader)\n",
        "  avg_val_loss = val_loss / len(test_loader)\n",
        "\n",
        "  print(f'Epoch {epoch+1}:')\n",
        "  print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "  print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "  print('=' * 50)\n"
      ],
      "metadata": {
        "id": "lfVcjE3aQvLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}