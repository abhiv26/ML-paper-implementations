{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySIspDofWtge"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDceLtuMXCfG"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    d_k = k.size(-1) # Q and K have shape (batch_size, n heads, seq_len, head dim)\n",
        "    scores = q@k.transpose(-2, -1) / math.sqrt(d_k) # (batch_size, n heads, q_len, k_len)\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attn = F.softmax(scores, dim=-1) # use last dim because we want to sum across keys to get probs for each query\n",
        "    attn = attn@v # (batch_size, n_heads, q_len, head dim)\n",
        "    return attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iP93Modpj_e"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_head = d_model // n_heads\n",
        "    self.W_q = nn.Linear(d_model, d_model) # d model is the embedding dimension\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    q = self.W_q(q) # q @ W.T, so shape of all of these is (batch size, seq len, embedding dim)\n",
        "    k = self.W_k(k)\n",
        "    v = self.W_v(v)\n",
        "\n",
        "    batch_size_q, seq_len_q, d_model_q = q.shape # seq len means # of tokens\n",
        "    batch_size_k, seq_len_k, d_model_k = k.shape\n",
        "    batch_size_v, seq_len_v, d_model_v = v.shape\n",
        "\n",
        "    # Now I need to perform multiplication across all heads, so I should reshape\n",
        "    # my q and k matrices into (batch size, n_heads, seq_len, head dimension)\n",
        "    # But we also need to give each feature its own axis\n",
        "    # Reshape itself will only give you the right # of elements in the right shape, but not the correct layout\n",
        "    q = q.reshape(batch_size_q, seq_len_q, self.n_heads, self.d_head).transpose(1, 2)\n",
        "      # The original shape of Q here is (batch size, seq len, embedding dim)\n",
        "      # We want to split the embedding dim into n_heads x d_head, or dimension of each head for a number of heads\n",
        "    k = k.reshape(batch_size_k, seq_len_k, self.n_heads, self.d_head).transpose(1, 2)\n",
        "    v = v.reshape(batch_size_v, seq_len_v, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "    attn = ScaledDotProductAttention()(q, k, v, mask) # Shape (batch size, # of heads, seq_len, # embedding dim per head)\n",
        "    attn_concat = attn.transpose(1, 2).reshape(batch_size_q, seq_len_q, d_model_q) # return axes to original posns before reshaping\n",
        "    mha_output = self.W_o(attn_concat)\n",
        "\n",
        "    return mha_output # shape (batch size, seq len, embedding dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqJrlfYeW9zA"
      },
      "outputs": [],
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, d_model, eps=1e-6):\n",
        "    super().__init__()\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True) # keep dim to maintain tensor dimensionality\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "# In batch normalization, each feature is normalized across all samples\n",
        "# In layer normalization, all features are normalized across each sample"
      ],
      "metadata": {
        "id": "A47YX8GUoFXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rksoaJc7Ycne"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_len):\n",
        "    super().__init__()\n",
        "    encoding_matrix = torch.zeros(max_seq_len, d_model)\n",
        "    position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
        "    # Each index in positional encoding accounts for TWO dimensions in an embedding vector\n",
        "    # What I do here is create the denominator term for the even set of indices,\n",
        "    # then apply it to both even and odd dimensions in the embeding\n",
        "    encoding_matrix[:, 0::2] = torch.sin(position*div_term) # even dims, start at 0 stepsize 2\n",
        "    encoding_matrix[:, 1::2] = torch.cos(position*div_term) # odd dims, start at 1 stepsize 2\n",
        "    self.register_buffer(\"pos_encoding\", encoding_matrix) # No need to train positional encoding\n",
        "  def forward(self, x):\n",
        "    return x + self.pos_encoding[:x.size(1)] # Return positional encodings for the target sequence length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP8AWL1PlSVp"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z1 = self.linear1(x)\n",
        "    a1 = F.relu(z1)\n",
        "    z2 = self.linear2(a1)\n",
        "    return z2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wywZ8fKnnKYp"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "    self.ff = FeedForward(d_model, d_ff)\n",
        "    self.layer1norm = LayerNorm(d_model)\n",
        "    self.layer2norm = LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = self.layer1norm(x + self.dropout(self.mha(x, x, x, src_mask)))\n",
        "    x = self.layer2norm(x + self.dropout(self.ff(x)))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzxQTFMNv_Bv"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.masked_attention = MultiHeadAttention(d_model, n_heads)\n",
        "    self.cross_attention = MultiHeadAttention(d_model, n_heads)\n",
        "    self.ff = FeedForward(d_model, d_ff)\n",
        "    self.norm1 = LayerNorm(d_model)\n",
        "    self.norm2 = LayerNorm(d_model)\n",
        "    self.norm3 = LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, encoder_output, tgt_mask):\n",
        "    x = self.norm1(x + self.dropout(self.masked_attention(x, x, x, tgt_mask)))# tgt mask is a causal mask for the decoder masked attention\n",
        "    x = self.norm2(x + self.dropout(self.cross_attention(x, encoder_output, encoder_output))) # src mask is an optional mask applied to encoder output for cross attention\n",
        "    x = self.norm3(x + self.dropout(self.ff(x)))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FflrZzMvTLpp"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, n_layers, src_vocab_size, tgt_vocab_size, max_seq_len, dropout_rate, padding_index=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.padding_idx = padding_index\n",
        "\n",
        "    self.encoder_embedding = InputEmbedding(src_vocab_size, d_model)\n",
        "    self.decoder_embedding = InputEmbedding(tgt_vocab_size, d_model)\n",
        "\n",
        "    self.positional_encoding_enc = PositionalEncoding(d_model, max_seq_len)\n",
        "    self.positional_encoding_dec = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "    self.encoders = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout_rate) for _ in range(n_layers)])\n",
        "    self.decoders = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout_rate) for _ in range(n_layers)])\n",
        "\n",
        "    self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "\n",
        "    # Create target padding mask\n",
        "    tgt_padding_mask = (tgt != self.padding_idx).unsqueeze(1).unsqueeze(2)  # shape: (batch_size, 1, 1, tgt_seq_len)\n",
        "\n",
        "    # Create target causal mask\n",
        "    tgt_len = tgt.size(1)\n",
        "    causal_mask = torch.tril(torch.ones(tgt_len, tgt_len)).to(tgt.device)  # shape: (tgt_len, tgt_len)\n",
        "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # shape: (1, 1, tgt_len, tgt_len)\n",
        "\n",
        "    # Combine both\n",
        "    tgt_mask = tgt_padding_mask.type(torch.bool) & causal_mask.type(torch.bool)\n",
        "\n",
        "    # source padding mask\n",
        "    src_padding_mask = (src != self.padding_idx).unsqueeze(1).unsqueeze(2) # source mask creates a mask that ignores padding tokens\n",
        "    # unsqueeze 1, 2 since we want to match key matrix\n",
        "\n",
        "    enc_input = self.encoder_embedding(src)\n",
        "    dec_input = self.decoder_embedding(tgt)\n",
        "\n",
        "    enc_input = self.positional_encoding_enc(enc_input) # shape batch size x src seq len x embedding dim\n",
        "    dec_input = self.positional_encoding_dec(dec_input) # shape batch size x tgt seq len x embedding dim\n",
        "\n",
        "    for encoder in self.encoders:\n",
        "      enc_input = encoder(enc_input, src_padding_mask) # each encoder builds off prev encoder, so we reassign to enc input, same for decoder\n",
        "\n",
        "    for decoder in self.decoders:\n",
        "      dec_input = decoder(dec_input, enc_input, tgt_mask)\n",
        "\n",
        "    logits = self.linear(dec_input)\n",
        "    return logits # Return logits here because in training cross entropy loss expects logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVTIplH9hPnS"
      },
      "outputs": [],
      "source": [
        "model = Transformer(\n",
        "    d_model=512,\n",
        "    n_heads=8,\n",
        "    d_ff=2048,\n",
        "    n_layers=6,\n",
        "    src_vocab_size=10000,\n",
        "    tgt_vocab_size=10000,\n",
        "    max_seq_len=100,\n",
        "    dropout_rate=0.1,\n",
        "    padding_index=0\n",
        ")\n",
        "\n",
        "batch_size = 2\n",
        "src_seq_len = 10\n",
        "tgt_seq_len = 9\n",
        "\n",
        "src = torch.randint(4, 1000, (batch_size, src_seq_len))\n",
        "tgt = torch.randint(4, 1000, (batch_size, tgt_seq_len))\n",
        "\n",
        "src[:, -2:] = 0\n",
        "tgt[:, -1:] = 0\n",
        "\n",
        "logits = model(src, tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vxT1lyRCciZ"
      },
      "outputs": [],
      "source": [
        "def test_transformer_forward_pass_doesnt_crash():\n",
        "    model = Transformer(\n",
        "        d_model=512,\n",
        "        n_heads=8,\n",
        "        d_ff=2048,\n",
        "        n_layers=2,\n",
        "        src_vocab_size=1000,\n",
        "        tgt_vocab_size=1000,\n",
        "        max_seq_len=50,\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "    src = torch.randint(1, 1000, (4, 32))  # batch size 4, src seq len 32\n",
        "    tgt = torch.randint(1, 1000, (4, 16))  # batch size 4, tgt seq len 16\n",
        "\n",
        "    out = model(src, tgt)\n",
        "    assert out.shape == (4, 16, 1000), f\"Unexpected output shape: {out.shape}\"\n",
        "\n",
        "test_transformer_forward_pass_doesnt_crash()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6e4uvNwChMV"
      },
      "outputs": [],
      "source": [
        "def test_padding_mask_blocks_attention():\n",
        "    model = Transformer(\n",
        "        d_model=128,\n",
        "        n_heads=4,\n",
        "        d_ff=512,\n",
        "        n_layers=1,\n",
        "        src_vocab_size=50,\n",
        "        tgt_vocab_size=50,\n",
        "        max_seq_len=10,\n",
        "        dropout_rate=0.0\n",
        "    )\n",
        "\n",
        "    pad_idx = 0\n",
        "    model.padding_idx = pad_idx\n",
        "\n",
        "    src = torch.tensor([\n",
        "        [1, 2, 3, 4, pad_idx, pad_idx],  # sequence with padding\n",
        "        [5, 6, 7, 8, 9, 10],             # no padding\n",
        "    ])\n",
        "    tgt = torch.tensor([\n",
        "        [1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "    ])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(src, tgt)\n",
        "\n",
        "    assert not torch.isnan(out).any(), \"Model output contains NaNs. Probably broken masking.\"\n",
        "\n",
        "test_padding_mask_blocks_attention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5tuy_haCtGd"
      },
      "outputs": [],
      "source": [
        "def test_causal_mask_blocks_future():\n",
        "    model = Transformer(\n",
        "        d_model=64,\n",
        "        n_heads=4,\n",
        "        d_ff=128,\n",
        "        n_layers=1,\n",
        "        src_vocab_size=100,\n",
        "        tgt_vocab_size=100,\n",
        "        max_seq_len=5,\n",
        "        dropout_rate=0.0\n",
        "    )\n",
        "\n",
        "    src = torch.randint(1, 100, (1, 5))  # batch size 1\n",
        "    tgt = torch.randint(1, 100, (1, 5))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(src, tgt)  # shape: (1, 5, vocab_size)\n",
        "\n",
        "    # Compare logits: position 1 should not be influenced by position 2\n",
        "    # (We'd need to dig into the attention weights to really prove this, but...)\n",
        "    assert output.shape == (1, 5, 100), \"Unexpected output shape in causal mask test.\"\n",
        "\n",
        "test_causal_mask_blocks_future()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuzRM3E3Cwvg"
      },
      "outputs": [],
      "source": [
        "def test_mask_shapes_align():\n",
        "    tgt_len = 6\n",
        "    tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len))\n",
        "    expanded = tgt_mask.unsqueeze(0).unsqueeze(1)  # shape should be (1, 1, tgt_len, tgt_len)\n",
        "    assert expanded.shape == (1, 1, tgt_len, tgt_len), f\"Mask shape is wrong: {expanded.shape}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUSqRjsqCyuO"
      },
      "outputs": [],
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(\n",
        "    d_model=d_model,\n",
        "    n_heads=num_heads,\n",
        "    d_ff=d_ff,\n",
        "    n_layers=num_layers,\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    max_seq_len=max_seq_length,\n",
        "    dropout_rate=dropout\n",
        ")\n",
        "\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoHZU6ROFtLR",
        "outputId": "b9e80f14-45ae-4951-db9e-cf65dac0c264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0) # ignore padding tokens\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(10):\n",
        "  optimizer.zero_grad()\n",
        "  output = transformer(src_data, tgt_data[:, :-1]) # input is all tokens except last, according to paper\n",
        "  # Target is all tokens except the first one (shifted right)\n",
        "  target = tgt_data[:, 1:].contiguous().view(-1)\n",
        "  output = output.contiguous().view(-1, output.size(-1))\n",
        "  loss = loss_fn(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
